import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.compose import ColumnTransformer, make_column_selector
from sklearn.ensemble import HistGradientBoostingRegressor, VotingRegressor
from sklearn.impute import SimpleImputer
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.model_selection import TimeSeriesSplit, cross_val_score
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import OneHotEncoder, StandardScaler, FunctionTransformer
from xgboost import XGBRegressor
from lightgbm import LGBMRegressor
import warnings
from matplotlib import rcParams
#è®¾ç½®å­—ä½“ä¸ºæ”¯æŒä¸­æ–‡
rcParams['font.sans-serif'] = ['SimHei']
rcParams['axes.unicode_minus'] = False

# é…ç½®å…¨å±€å‚æ•°
class Config:
    DATE_COL = 'Date'
    TARGET_COL = 'PriceMedian'
    RANDOM_STATE = 42
    TS_CV_SPLITS = 5
    PLOT_STYLE = 'seaborn-v0_8'
    FIG_SIZE = (12, 6)
    USE_PARALLEL = True  # å…¨å±€å¹¶è¡Œæ§åˆ¶
    N_JOBS = -1  # å¹¶è¡Œå·¥ä½œæ•°


# å¿½ç•¥è­¦å‘Š
warnings.filterwarnings('ignore')
plt.style.use(Config.PLOT_STYLE)


class DataLoader:
    """æ™ºèƒ½æ•°æ®åŠ è½½ä¸é¢„å¤„ç†"""

    @staticmethod
    def load_data(filepath):
        """ä¼˜åŒ–åçš„æ•°æ®åŠ è½½ç®¡é“"""
        df = (
            pd.read_csv(filepath, parse_dates=[Config.DATE_COL])
            .pipe(DataLoader._basic_cleaning)
            .pipe(DataLoader._add_temporal_features)
            .pipe(DataLoader._handle_outliers)
            .pipe(DataLoader._encode_categories)
        )
        return df

    @staticmethod
    def _basic_cleaning(df):
        """åŸºç¡€æ•°æ®æ¸…æ´—"""
        return df.assign(
            PriceMedian=lambda x: (x['High Price'] + x['Low Price']) / 2,
            Package=lambda x: x['Package'].str[:30]  # æˆªæ–­é•¿æ–‡æœ¬
        ).dropna(subset=['PriceMedian'])

    @staticmethod
    def _add_temporal_features(df):
        """æ·»åŠ æ—¶åºç‰¹å¾"""
        return df.assign(
            Year=lambda x: x[Config.DATE_COL].dt.year,
            Month=lambda x: x[Config.DATE_COL].dt.month,
            DayOfYear=lambda x: x[Config.DATE_COL].dt.dayofyear,
            WeekOfYear=lambda x: x[Config.DATE_COL].dt.isocalendar().week,
            Month_sin=lambda x: np.sin(2 * np.pi * x[Config.DATE_COL].dt.month / 12),
            Month_cos=lambda x: np.cos(2 * np.pi * x[Config.DATE_COL].dt.month / 12),
            DayOfWeek=lambda x: x[Config.DATE_COL].dt.dayofweek,
            IsWeekend=lambda x: x[Config.DATE_COL].dt.dayofweek.isin([5, 6]).astype(int)
        )

    @staticmethod
    def _handle_outliers(df):
        """å¤„ç†å¼‚å¸¸å€¼"""
        q_low = df['PriceMedian'].quantile(0.01)
        q_high = df['PriceMedian'].quantile(0.99)
        return df[df['PriceMedian'].between(q_low, q_high)]

    @staticmethod
    def _encode_categories(df):
        """åˆ†ç±»å˜é‡é¢„å¤„ç†"""
        df['Variety'] = df['Variety'].where(
            df['Variety'].map(df['Variety'].value_counts()) > 50,
            'Other'
        )
        return df


class FeatureAnalyzer:
    """ç‰¹å¾åˆ†æä¸å¯è§†åŒ–"""

    @staticmethod
    def plot_feature_distributions(df):
        """ç»˜åˆ¶ç‰¹å¾åˆ†å¸ƒ"""
        num_cols = df.select_dtypes(include=np.number).columns.tolist()

        plt.figure(figsize=Config.FIG_SIZE)
        sns.pairplot(df[num_cols[:5]], diag_kind='kde')
        plt.suptitle('æ•°å€¼ç‰¹å¾åˆ†å¸ƒçŸ©é˜µ', y=1.02)
        plt.show()

        plt.figure(figsize=Config.FIG_SIZE)
        sns.heatmap(df[num_cols].corr(), annot=True, fmt=".2f", cmap='coolwarm')
        plt.title('ç‰¹å¾ç›¸å…³æ€§çŸ©é˜µ')
        plt.show()

    @staticmethod
    def analyze_temporal_trends(df):
        """åˆ†ææ—¶åºè¶‹åŠ¿"""
        fig, ax = plt.subplots(2, 1, figsize=(Config.FIG_SIZE[0], Config.FIG_SIZE[1] * 1.5))

        # æœˆåº¦è¶‹åŠ¿
        monthly = df.groupby('Month')['PriceMedian'].agg(['mean', 'std'])
        monthly.plot(kind='bar', y='mean', yerr='std', ax=ax[0], capsize=4)
        ax[0].set_title('æœˆåº¦ä»·æ ¼è¶‹åŠ¿')

        # å“ç§è¶‹åŠ¿
        sns.boxplot(x='Month', y='PriceMedian', hue='Variety',
                    data=df[df['Variety'].isin(df['Variety'].value_counts().index[:3])],
                    ax=ax[1])
        ax[1].set_title('ä¸»è¦å“ç§æœˆåº¦ä»·æ ¼åˆ†å¸ƒ')
        plt.tight_layout()
        plt.show()


class FeatureProcessor:
    """ç‰¹å¾å·¥ç¨‹å¤„ç†å™¨"""

    @staticmethod
    def get_feature_pipeline():
        """åˆ›å»ºç‰¹å¾å¤„ç†ç®¡é“"""
        # æ•°å€¼ç‰¹å¾å¤„ç†
        num_pipeline = make_pipeline(
            SimpleImputer(strategy='median'),
            StandardScaler()
        )

        # åˆ†ç±»ç‰¹å¾å¤„ç†
        cat_pipeline = make_pipeline(
            SimpleImputer(strategy='most_frequent'),
            OneHotEncoder(handle_unknown='infrequent_if_exist', sparse_output=False),
            StandardScaler()
        )

        # è‡ªåŠ¨ç‰¹å¾é€‰æ‹©
        preprocessor = ColumnTransformer([
            ('num', num_pipeline, make_column_selector(dtype_include=np.number)),
            ('cat', cat_pipeline, ['Variety', 'Package', 'City Name'])
        ])

        return preprocessor


class ModelBuilder:
    """æ¨¡å‹æ„å»ºä¸è¯„ä¼°"""

    @staticmethod
    def _get_parallel_params():
        """è·å–å¹¶è¡Œè®¡ç®—å‚æ•°"""
        return {'n_jobs': Config.N_JOBS} if Config.USE_PARALLEL else {}

    @staticmethod
    def build_models():
        """æ„å»ºæ¨¡å‹é›†åˆ"""
        base_params = {
            'random_state': Config.RANDOM_STATE
        }

        parallel_params = ModelBuilder._get_parallel_params()

        return {
            'HGB': HistGradientBoostingRegressor(
                learning_rate=0.05,
                max_iter=200,
                **base_params
            ),
            'XGB': XGBRegressor(
                n_estimators=150,
                subsample=0.8,
                tree_method='hist',
                **base_params,
                **parallel_params
            ),
            'LGBM': LGBMRegressor(
                n_estimators=120,
                subsample=0.7,
                **base_params,
                **parallel_params
            ),
            'Ensemble': VotingRegressor([
                ('hgb', HistGradientBoostingRegressor(**base_params)),
                ('xgb', XGBRegressor(**base_params, **parallel_params))
            ])
        }

    @staticmethod
    def evaluate_model(model, X, y):
        """è¯„ä¼°æ¨¡å‹æ€§èƒ½"""
        tscv = TimeSeriesSplit(n_splits=Config.TS_CV_SPLITS)
        pipeline = make_pipeline(FeatureProcessor.get_feature_pipeline(), model)

        metrics = {
            'MAE': cross_val_score(pipeline, X, y, cv=tscv,
                                   scoring='neg_mean_absolute_error'),
            'R2': cross_val_score(pipeline, X, y, cv=tscv, scoring='r2')
        }

        return {k: {'mean': np.mean(v), 'std': np.std(v)} for k, v in metrics.items()}

    @staticmethod
    def plot_feature_importance(model, feature_names):
        """å¯è§†åŒ–ç‰¹å¾é‡è¦æ€§"""
        if hasattr(model, 'feature_importances_'):
            importance = pd.Series(
                model.feature_importances_,
                index=feature_names
            ).sort_values(ascending=False)

            plt.figure(figsize=Config.FIG_SIZE)
            importance.head(15).plot(kind='barh')
            plt.title('Top 15 é‡è¦ç‰¹å¾')
            plt.show()


def main():
    # 1. æ•°æ®åŠ è½½
    print("â³ åŠ è½½æ•°æ®...")
    df = DataLoader.load_data('US-pumpkins.csv')
    print(f"âœ… æ•°æ®åŠ è½½å®Œæˆï¼Œå…± {len(df)} æ¡è®°å½•")

    # 2. ç‰¹å¾åˆ†æ
    print("\nğŸ” åˆ†æç‰¹å¾åˆ†å¸ƒ...")
    FeatureAnalyzer.plot_feature_distributions(df)
    FeatureAnalyzer.analyze_temporal_trends(df)

    # 3. å‡†å¤‡å»ºæ¨¡æ•°æ®
    X = df.drop(columns=[Config.TARGET_COL, Config.DATE_COL])
    y = df[Config.TARGET_COL]

    # 4. æ¨¡å‹è®­ç»ƒä¸è¯„ä¼°
    print("\nğŸ¤– è®­ç»ƒæ¨¡å‹ä¸­...")
    models = ModelBuilder.build_models()
    results = []

    for name, model in models.items():
        scores = ModelBuilder.evaluate_model(model, X, y)
        results.append({
            'Model': name,
            'MAE': f"{abs(scores['MAE']['mean']):.2f} Â±{scores['MAE']['std']:.2f}",
            'R2': f"{scores['R2']['mean']:.3f} Â±{scores['R2']['std']:.3f}"
        })

    # 5. ç»“æœå±•ç¤º
    results_df = pd.DataFrame(results)
    print("\nğŸ“Š æ¨¡å‹æ€§èƒ½å¯¹æ¯”:")
    print(results_df.sort_values('R2', ascending=False))

    # 6. æœ€ä½³æ¨¡å‹åˆ†æ
    best_model = make_pipeline(
        FeatureProcessor.get_feature_pipeline(),
        models['Ensemble']
    )
    best_model.fit(X, y)

    try:
        feature_names = best_model[0].get_feature_names_out()
        ModelBuilder.plot_feature_importance(best_model[-1], feature_names)
    except Exception as e:
        print(f"âš ï¸ ç‰¹å¾é‡è¦æ€§å¯è§†åŒ–å¤±è´¥: {str(e)}")


if __name__ == "__main__":
    main()